# VOA_Crawler.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
import re
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
import crawler_utils  # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
import crawler_config # ğŸ‘ˆ ì„¤ì • íŒŒì¼ ì„í¬íŠ¸

# --- â¬‡ï¸ ê³µí†µ ì½”ë“œ (ì‚­ì œ ë° utilsë¡œ ëŒ€ì²´) â¬‡ï¸ ---
NEWS_JSON_DIR = 'news_json'
result_filename = os.path.join(NEWS_JSON_DIR, 'voa_News.json') # ğŸ‘ˆ ê³ ìœ ê°’

# 1. ê³µí†µ ìœ í‹¸ë¦¬í‹°ì—ì„œ í‚¤ì›Œë“œì™€ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
keywords, exclude_keywords = crawler_utils.load_keywords()
today = crawler_utils.get_today_string()

# 2. ê³ ìœ í•œ URL ë¦¬ìŠ¤íŠ¸
urls = [
    'https://www.voakorea.com/z/2767',  # ì •ì¹˜ì•ˆë³´
    'https://www.voakorea.com/z/2768',  # ê²½ì œì§€ì›
    'https://www.voakorea.com/z/2769',  # ì‚¬íšŒì¸ê¶Œ
    'https://www.voakorea.com/z/2824',  # ì¤‘ë™
    'https://www.voakorea.com/z/6936',  # ìš°í¬ë¼ì´ë‚˜
    'https://www.voakorea.com/z/2698'   # ì„¸ê³„
] # ğŸ‘ˆ ê³ ìœ ê°’

processed_links = set()

# 3. is_relevant_article, get_existing_links, save_to_json í•¨ìˆ˜
# (ì´ íŒŒì¼ì—ì„œ ëª¨ë‘ ì‚­ì œ -> crawler_utilsê°€ ëŒ€ì‹  ì²˜ë¦¬)

# --- â¬‡ï¸ ì´ í¬ë¡¤ëŸ¬ë§Œì˜ 'ê³ ìœ í•œ' ë¡œì§ (ê·¸ëŒ€ë¡œ ë‘ ) â¬‡ï¸ ---

def extract_article_details(url):
    """(ê³ ìœ  ë¡œì§) ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        summary_element = soup.select_one('p.perex, p[class*="perex"]') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
        summary = summary_element.text.strip() if summary_element else ''
        print(f"URL: {url}, ìš”ì•½: {summary}")
        return summary
    except Exception as e:
        print(f"ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return ''

def process_article(element):
    """(ê³ ìœ  ë¡œì§)"""
    href_link = element.find('a')['href']
    if not href_link.startswith('http'):
        href_link = 'https://www.voakorea.com' + href_link
    
    if href_link in processed_links:
        print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬: {href_link}")
        return None
    
    title_element = element.find('h4', class_='media-block__title') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
    text_content = title_element.text.strip() if title_element else ''
    
    summary = extract_article_details(href_link)
    full_text = f"{text_content} {summary}"
    
    # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
    if not crawler_utils.is_relevant(full_text, keywords, exclude_keywords):
        print(f"ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬: {text_content}")
        return None
    
    time_element = element.find('span', class_='date') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
    published_time = time_element.text.strip() if time_element else ''
    try:
        # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: 2025ë…„ 3ì›” 16ì¼) # ğŸ‘ˆ ê³ ìœ  ì‹œê°„ íŒŒì‹±
        parsed_time = datetime.strptime(published_time, '%Yë…„ %mì›” %dì¼')
        formatted_time = parsed_time.replace(hour=0, minute=0, second=0).isoformat()
    except ValueError as e:
        print(f"ì˜ëª»ëœ ì‹œê°„ í˜•ì‹: {published_time}, ì—ëŸ¬: {e}")
        return None
    
    img_element = element.find('img')
    img_url = img_element.get('src') if img_element else ''
    if img_url and not img_url.startswith('http'):
        img_url = 'https://www.voakorea.com' + img_url
    
    processed_links.add(href_link)
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬: {text_content} ({formatted_time})")
    return {
        'title': text_content,
        'time': formatted_time,
        'img': img_url,
        'url': href_link,
        #'original_url': href_link,
        'summary': summary
    }

def scrape_page(url):
    """(ê³ ìœ  ë¡œì§)"""
    print(f"Scraping URL: {url}")
    articles = []
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        relevant_elements = soup.select('div.media-block') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
        print(f"ì„ íƒëœ ìš”ì†Œ ìˆ˜: {len(relevant_elements)}")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(process_article, element) for element in relevant_elements]
            for future in as_completed(futures):
                article = future.result()
                if article:
                    articles.append(article)
        
        return articles
    except Exception as e:
        print(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
        return []

# --- â¬‡ï¸ main í•¨ìˆ˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) â¬‡ï¸ ---

def main():
    global processed_links
    
    # 1. ê³µí†µ í•¨ìˆ˜ë¡œ íŒŒì¼ ìƒì„± ë° ê¸°ì¡´ ë§í¬ ë¡œë“œ
    #crawler_utils.ensure_file_exists(result_filename)
    processed_links = crawler_utils.get_existing_links(result_filename)
    
    all_articles = []
    
    # 2. ê³ ìœ í•œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì‹¤í–‰
    for url in urls:
        articles = scrape_page(url)
        all_articles.extend(articles)
    
    # 3. ê³µí†µ í•¨ìˆ˜ë¡œ ì €ì¥
    if all_articles:
        crawler_utils.save_articles_to_json(result_filename, all_articles, today)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í•¨")

if __name__ == "__main__":
    main()
