# Naver_Crawler.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
import re
import crawler_utils  # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸

# --- â¬‡ï¸ ê³µí†µ ì½”ë“œ (ëŒ€ë¶€ë¶„ ì‚­ì œë¨) â¬‡ï¸ ---
NEWS_JSON_DIR = 'news_json'
result_filename = os.path.join(NEWS_JSON_DIR, 'naver_News.json') # ğŸ‘ˆ ê³ ìœ ê°’

# 1. ê³µí†µ ìœ í‹¸ë¦¬í‹°ì—ì„œ í‚¤ì›Œë“œì™€ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
keywords, exclude_keywords = crawler_utils.load_keywords()
today = crawler_utils.get_today_string()

# 2. ê³ ìœ í•œ URL ë¦¬ìŠ¤íŠ¸
urls = [
    'https://news.naver.com/section/100',  # ì •ì¹˜
    'https://news.naver.com/section/101',  # ê²½ì œ
    'https://news.naver.com/section/103',  # ìƒí™œ/ë¬¸í™”
    'https://news.naver.com/section/104',  # ì„¸ê³„
    'https://news.naver.com/section/105',  # IT/ê³¼í•™
    'https://news.naver.com/breakingnews/section/104/231',  # ì•„ì‹œì•„/í˜¸ì£¼
    'https://news.naver.com/breakingnews/section/104/232',  # ìœ ëŸ½
    'https://news.naver.com/breakingnews/section/104/233',  # ì¤‘ë‚¨ë¯¸
    'https://news.naver.com/breakingnews/section/104/234',  # ì¤‘ë™/ì•„í”„ë¦¬ì¹´
    'https://news.naver.com/breakingnews/section/104/322',  # ë¶ë¯¸
] # ğŸ‘ˆ ê³ ìœ ê°’

processed_links = set()
processed_titles = set()

# 3. is_relevant_article í•¨ìˆ˜ -> ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš© (ì‚­ì œë¨)
# 4. get_existing_links í•¨ìˆ˜ -> ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš© (ì‚­ì œë¨)

# --- â¬‡ï¸ ì´ í¬ë¡¤ëŸ¬ë§Œì˜ 'ê³ ìœ í•œ' ë¡œì§ (ê·¸ëŒ€ë¡œ ë‘ ) â¬‡ï¸ ---

def extract_article_details(url):
    """ë„¤ì´ë²„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ê³ ìœ  ë¡œì§)"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
        time_element = soup.select_one('span[class*="ARTICLE_DATE_TIME"]')
        published_time = ''
        if time_element:
            published_time_data = time_element.get('data-date-time', '')
            if published_time_data:
                try:
                    dt = datetime.strptime(published_time_data, '%Y-%m-%d %H:%M:%S')
                    published_time = dt.isoformat()
                except ValueError as e:
                    print(f"Invalid time format: {published_time_data}, Error: {e}")
                    return '', '', ''
        
        # ìš”ì•½ ì •ë³´ ì¶”ì¶œ
        summary_element = soup.select_one('article#dic_area strong[style*="border-left: 2px solid"]')
Â  Â  Â  Â  
Â  Â  Â  Â  summary = ''
        
        # 1. ì²« ë²ˆì§¸ ì¼€ì´ìŠ¤ ì‹œë„: .media_end_summary (ê¸°ì¡´ì— ì‘ë™í•˜ë˜ ë°©ì‹)
        #    ì´ ì¼€ì´ìŠ¤ëŠ” <br> íƒœê·¸ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        summary_element_1 = soup.select_one('.media_end_summary')
        
        if summary_element_1:
            summary_html = summary_element_1.decode_contents()
            summary = summary_html.replace('<br>', '\n').replace('<br/>', '\n').strip()
            
        # 2. ì²« ë²ˆì§¸ ì¼€ì´ìŠ¤ê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ (summaryê°€ ì—¬ì „íˆ ë¹„ì–´ìˆë‹¤ë©´), 
        #    ë‘ ë²ˆì§¸ ì¼€ì´ìŠ¤ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
        if not summary:
            #    ì´ ì¼€ì´ìŠ¤ëŠ” ë‹¨ìˆœ <strong> íƒœê·¸ì…ë‹ˆë‹¤.
            summary_element_2 = soup.select_one('article#dic_area strong[style*="border-left: 2px solid"]')
            
            if summary_element_2:
                summary = summary_element_2.get_text(strip=True)
        
        # --- â¬†ï¸ ìš”ì•½ ì •ë³´ ì¶”ì¶œ ë â¬†ï¸ ---
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        img_element = soup.select_one('img#img1')
        img_url = img_element.get('data-src', '') if img_element else ''
        
        return published_time, img_url, summary
    except Exception as e:
        print(f"ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return '', '', ''

def scrape_page(url):
    """ë„¤ì´ë²„ ì„¹ì…˜ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (ê³ ìœ  ë¡œì§)"""
    print(f"Scraping URL: {url}")
    articles = []
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        article_elements = soup.select('div.section_latest_article ul li')
        print(f"Found {len(article_elements)} articles")
        
        for element in article_elements:
            title_element = element.select_one('div.sa_text a strong')
            if title_element:
                text_content = title_element.get_text(strip=True)
                href_link = title_element.parent['href']
                full_link = href_link if href_link.startswith('http') else f'https://news.naver.com{href_link}'
                
                # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
                is_relevant = crawler_utils.is_relevant(text_content, keywords, exclude_keywords)
                
                if full_link not in processed_links and text_content not in processed_titles and is_relevant:
                    published_time, img_url, summary = extract_article_details(full_link)
                    if published_time:
                        processed_links.add(full_link)
                        processed_titles.add(text_content)
                        articles.append({
                            'title': text_content,
                            'time': published_time,
                            'img': img_url,
                            'url': full_link,
                            #'original_url': full_link,
                            'summary': summary
                        })
                        print(f"Article processed: {text_content} ({published_time})")
    except Exception as e:
        print(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
    return articles

# 5. save_to_json í•¨ìˆ˜ -> ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš© (ì‚­ì œë¨)

# --- â¬‡ï¸ main í•¨ìˆ˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) â¬‡ï¸ ---

def main():
    global processed_links, processed_titles
    
    # 1. ê³µí†µ í•¨ìˆ˜ë¡œ íŒŒì¼ ìƒì„± ë° ê¸°ì¡´ ë§í¬ ë¡œë“œ
    #crawler_utils.ensure_file_exists(result_filename)
    processed_links = crawler_utils.get_existing_links(result_filename)
    
    all_articles = []
    
    # 2. ê³ ìœ í•œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì‹¤í–‰
    for url in urls:
        articles = scrape_page(url)
        all_articles.extend(articles)
    
    # 3. ê³µí†µ í•¨ìˆ˜ë¡œ ì €ì¥
    if all_articles:
        crawler_utils.save_articles_to_json(result_filename, all_articles, today)
    else:
        print("No new articles found")

if __name__ == "__main__":
    main()
