# SkyDaily_Crawler.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
import re
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import crawler_utils  # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
import crawler_config # ğŸ‘ˆ ì„¤ì • íŒŒì¼ ì„í¬íŠ¸ (is_relevant_articleì—ì„œ ì‚¬ìš©)

# --- â¬‡ï¸ ê³µí†µ ì½”ë“œ (ì‚­ì œ ë° utilsë¡œ ëŒ€ì²´) â¬‡ï¸ ---
NEWS_JSON_DIR = 'news_json'
result_filename = os.path.join(NEWS_JSON_DIR, 'skyDaily_News.json') # ğŸ‘ˆ ê³ ìœ ê°’

# 1. ê³µí†µ ìœ í‹¸ë¦¬í‹°ì—ì„œ í‚¤ì›Œë“œì™€ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
keywords, exclude_keywords = crawler_utils.load_keywords()
today = crawler_utils.get_today_string()

# 2. ê³ ìœ í•œ URL ë¦¬ìŠ¤íŠ¸
urls = [
    'https://www.skyedaily.com/news/articlelist.html?mode=list',  # ìµœì‹ ê¸°ì‚¬
    'https://www.skyedaily.com/news/news_list21.html',  # ì˜¤í”¼ë‹ˆì–¸
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=4',  # ì •ì¹˜
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=5',  # ì‚¬íšŒ
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=40',  # ê²½ì œ
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=2',  # ì‚°ì—…
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=51',  # ìƒí™œê²½ì œ
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=30',  # ê¸ˆìœµ
    'https://www.skyedaily.com/news/news_list30.html?mode=ct&m_section=6',  # ë¬¸í™”
] # ğŸ‘ˆ ê³ ìœ ê°’

processed_links = set()

# 3. is_relevant_article, get_existing_links, save_to_json í•¨ìˆ˜
# (ì´ íŒŒì¼ì—ì„œ ëª¨ë‘ ì‚­ì œ -> crawler_utilsê°€ ëŒ€ì‹  ì²˜ë¦¬)

# --- â¬‡ï¸ ì´ í¬ë¡¤ëŸ¬ë§Œì˜ 'ê³ ìœ í•œ' ë¡œì§ (ê·¸ëŒ€ë¡œ ë‘ ) â¬‡ï¸ ---

def extract_article_details(url):
    """(ê³ ìœ  ë¡œì§)"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr' # ğŸ‘ˆ SkyDaily ê³ ìœ  ì¸ì½”ë”©
        soup = BeautifulSoup(response.text, 'html.parser')
        summary_element = soup.select_one('div.article_txt')
        summary = summary_element.text.strip() if summary_element else ''
        #print(f"URL: {url}, ìš”ì•½: {summary[:50]}...")
        return summary
    except Exception as e:
        print(f"ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return ''

def process_article(element):
    """(ê³ ìœ  ë¡œì§)"""
    href_link = element.get('href')
    if not href_link.startswith('http'):
        href_link = 'https://www.skyedaily.com' + href_link
    
    if href_link in processed_links:
        # print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬: {href_link}")
        return None
    
    title_element = element.find('font', class_='sctionarticletitle')
    time_element = element.find_next('font', class_='picarticletxt')
    
    if not (title_element and time_element):
        # print(f"ì œëª© ë˜ëŠ” ì‹œê°„ ìš”ì†Œ ëˆ„ë½: {href_link}")
        return None
    
    title = title_element.text.strip()
    time_str = time_element.text.strip()
    
    try:
        # ê¸°ë³¸ ì‹œê°„ í˜•ì‹: 2025.03.16 12:34
        parsed_time = datetime.strptime(time_str, '%Y.%m.%d %H:%M')
        formatted_time = parsed_time.isoformat()
    except ValueError:
        try:
            # ëŒ€ì²´ í˜•ì‹: 2025.03.16
            parsed_time = datetime.strptime(time_str, '%Y.%m.%d')
            formatted_time = parsed_time.replace(hour=0, minute=0, second=0).isoformat()
        except ValueError:
            try:
                # í•œêµ­ì–´ í˜•ì‹: 2025ë…„ 3ì›” 16ì¼
                parsed_time = datetime.strptime(time_str, '%Yë…„ %mì›” %dì¼')
                formatted_time = parsed_time.replace(hour=0, minute=0, second=0).isoformat()
            except ValueError as e:
                print(f"ì˜ëª»ëœ ì‹œê°„ í˜•ì‹: {time_str}, ì—ëŸ¬: {e}")
                return None
    
    summary = extract_article_details(href_link)
    text_content = f"{title} {summary}"
    
    # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
    if not crawler_utils.is_relevant(text_content, keywords, exclude_keywords):
        #print(f"ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬: {title}")
        return None
    
    img_element = element.find('img')
    img_url = img_element.get('src') if img_element else ''
    if img_url and not img_url.startswith('http'):
        img_url = 'https://www.skyedaily.com' + img_url
    
    processed_links.add(href_link)
    print(f"ì²˜ë¦¬ëœ ê¸°ì‚¬: {title} ({formatted_time})")
    return {
        'title': title,
        'time': formatted_time,
        'img': img_url,
        'url': href_link,
        #'original_url': href_link,
        'summary': summary
    }

def scrape_page(url):
    """(ê³ ìœ  ë¡œì§)"""
    #print(f"Scraping URL: {url}")
    articles = []
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr' # ğŸ‘ˆ SkyDaily ê³ ìœ  ì¸ì½”ë”©
        soup = BeautifulSoup(response.text, 'html.parser')
        relevant_elements = soup.select('div.picarticle a') # ğŸ‘ˆ SkyDaily ê³ ìœ  ì„ íƒì
        print(f"ì„ íƒëœ ìš”ì†Œ ìˆ˜: {len(relevant_elements)}")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(process_article, element) for element in relevant_elements]
            for future in as_completed(futures):
                article = future.result()
                if article:
                    articles.append(article)
        
        return articles
    except Exception as e:
        print(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
        return []

# --- â¬‡ï¸ main í•¨ìˆ˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) â¬‡ï¸ ---

def main():
    global processed_links
    
    # 1. ê³µí†µ í•¨ìˆ˜ë¡œ íŒŒì¼ ìƒì„± ë° ê¸°ì¡´ ë§í¬ ë¡œë“œ
    #crawler_utils.ensure_file_exists(result_filename)
    processed_links = crawler_utils.get_existing_links(result_filename)
    
    all_articles = []
    
    # 2. ê³ ìœ í•œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì‹¤í–‰
    for url in urls:
        articles = scrape_page(url)
        all_articles.extend(articles)
        time.sleep(1)
    
    # 3. ê³µí†µ í•¨ìˆ˜ë¡œ ì €ì¥
    if all_articles:
        crawler_utils.save_articles_to_json(result_filename, all_articles, today)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í•¨")

if __name__ == "__main__":
    main()
