# Nate_Crawler.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse, urlunparse
import time
import crawler_utils  # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸

# --- â¬‡ï¸ ê³µí†µ ì½”ë“œ â¬‡ï¸ ---
NEWS_JSON_DIR = 'news_json'
result_filename = os.path.join(NEWS_JSON_DIR, 'nate_News.json') # ğŸ‘ˆ ê³ ìœ ê°’

keywords, exclude_keywords = crawler_utils.load_keywords()
today = crawler_utils.get_today_string()

# --- â¬‡ï¸ ê³ ìœ  ë¡œì§ â¬‡ï¸ ---
base_urls = [
    'https://news.nate.com/recent?mid=n0102',  # ê²½ì œ
    'https://news.nate.com/recent?mid=n0103',  # ì‚¬íšŒ
    'https://news.nate.com/recent?mid=n0104',  # ì„¸ê³„
    'https://news.nate.com/recent?mid=n0105',  # IT/ê³¼í•™
] # ğŸ‘ˆ ê³ ìœ ê°’

processed_links = set()
processed_titles = set()

def get_date_list():
    today_dt = datetime.now()
    return [today_dt.strftime('%Y%m%d')]

# --- â¬‡ï¸ 'Nate_Crawler.py'ì˜ 'get_nate_summary' í•¨ìˆ˜ë¥¼ í†µì§¸ë¡œ êµì²´í•˜ì„¸ìš” â¬‡ï¸ ---

def get_nate_summary(url):
    """
    [ìˆ˜ì •ë¨] Nate ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‘ ê°€ì§€ ìœ í˜•ì˜ ìš”ì•½ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ìœ í˜• 1: div.subArea.subTitle (ìƒˆë¡œ ë°œê²¬ëœ êµ¬ì¡°)
    ìœ í˜• 2: div#realArtcContents (ê¸°ì¡´ì˜ ì²« í…ìŠ¤íŠ¸ ë…¸ë“œ êµ¬ì¡°)
    """
    try:
        # 1. ìƒì„¸ í˜ì´ì§€ HTML ìš”ì²­
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        detail_soup = BeautifulSoup(response.text, 'html.parser')
        
        summary = ""

        # --- â¬‡ï¸ [ìˆ˜ì •ëœ ë¡œì§] â¬‡ï¸ ---

        # 1. (ì‹ ê·œ) 'ìœ í˜• 2' (subArea subTitle) ì¼€ì´ìŠ¤ ë¨¼ì € ì‹œë„
        #    ì´ ìš”ì†ŒëŠ” <br> íƒœê·¸ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        summary_element_1 = detail_soup.select_one('div.subArea.subTitle')
        
        if summary_element_1:
            summary_html = summary_element_1.decode_contents()
            summary = summary_html.replace('<br>', '\n').replace('<br/>', '\n').strip()

        # 2. (ê¸°ì¡´) 'ìœ í˜• 2'ê°€ ì—†ë‹¤ë©´, 'ìœ í˜• 1' (realArtcContents) ì‹œë„
        if not summary:
            content_area = detail_soup.select_one('div#realArtcContents')
            if content_area:
                # div#realArtcContents ë°”ë¡œ ì•„ë˜ì˜ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                for node in content_area.find_all(string=True, recursive=False):
                    summary_text = node.strip()
                    # ë¹„ì–´ìˆì§€ ì•Šê³ , ì£¼ì„ì´ ì•„ë‹Œ ì²« í…ìŠ¤íŠ¸
                    if summary_text and not summary_text.startswith('google_ad_section_start'):
                        summary = summary_text
                        break # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¢…ë£Œ
        
        # --- â¬†ï¸ [ë¡œì§ ìˆ˜ì • ì™„ë£Œ] â¬†ï¸ ---

        if not summary:
             print(f"Summary not found: No known summary structure matched on {url}")

        return summary
        
    except Exception as e:
        print(f"Nate ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return ""

def process_article(article, base_url):
    link_element = article.select_one('a.lt1')
    if not link_element:
        print("No link element found")
        return None
    
    href_link = link_element.get('href', '')
    if not href_link:
        print("No href in link element")
        return None
    
    full_link = urljoin(base_url, href_link)
    parsed_url = urlparse(full_link)
    clean_url = urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, '', '', ''))
    
    if clean_url in processed_links:
        print(f"Duplicate URL: {clean_url}")
        return None
    
    title_element = article.select_one('h2.tit')
    if not title_element:
        print("No title element found")
        return None
    
    text_content = title_element.get_text(strip=True)
    
    # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
    if text_content in processed_titles or not crawler_utils.is_relevant(text_content, keywords, exclude_keywords):
        return None
    
    time_element = article.select_one('span.medium em')
    published_time = time_element.get_text(strip=True) if time_element else ''
    if not published_time:
        print("No time element found")
        return None
    
    # ìœ ì—°í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
    try:
        if '-' in published_time:  # ì˜ˆ: 04-18 20:54
            parsed_time = datetime.strptime(published_time, '%m-%d %H:%M')
            parsed_time = parsed_time.replace(year=datetime.now().year)  # ì—°ë„ ì¶”ê°€
        else:  # ì˜ˆ: 2025.04.18 20:54
            parsed_time = datetime.strptime(published_time, '%Y.%m.%d %H:%M')
        formatted_time = parsed_time.isoformat()
    except ValueError as e:
        print(f"Invalid time format: {published_time}, Error: {e}")
        return None
    
    img_element = article.select_one('img')
    img_url = img_element.get('src', '') if img_element else ''

    summary = get_nate_summary(clean_url)
    
    processed_links.add(clean_url)
    processed_titles.add(text_content)
    print(f"Article processed: {text_content}")
    return {
        'title': text_content,
        'time': formatted_time,
        'img': img_url,
        'url': clean_url,
        #'original_url': clean_url,
        'summary': summary  # ğŸ‘ˆ ìš”ì•½ í•„ë“œ ì¶”ê°€
    }

def scrape_page(url):
    print(f"Scraping URL: {url}")
    articles = []
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        article_elements = soup.select('div.mlt01')
        print(f"Found {len(article_elements)} articles")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(process_article, article, url) for article in article_elements]
            for future in as_completed(futures):
                article = future.result()
                if article:
                    articles.append(article)
        
        return articles
    except Exception as e:
        print(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
        return []

# --- â¬‡ï¸ main í•¨ìˆ˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) â¬‡ï¸ ---
def main():
    global processed_links, processed_titles
    
    #crawler_utils.ensure_file_exists(result_filename)
    processed_links = crawler_utils.get_existing_links(result_filename)
    
    all_articles = []
    
    for base_url in base_urls:
        for date in get_date_list():
            page = 1
            while page <= 10:
                url = f'{base_url}&type=c&date={date}&page={page}'
                articles = scrape_page(url)
                all_articles.extend(articles)
                if not articles:
                    break
                page += 1
                time.sleep(1)
    
    if all_articles:
        crawler_utils.save_articles_to_json(result_filename, all_articles, today)
    else:
        print("No new articles found")

if __name__ == "__main__":
    main()
