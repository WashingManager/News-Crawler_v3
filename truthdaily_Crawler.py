# truthdaily_Crawler.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import os
import re
import time
import crawler_utils  # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
import crawler_config # ğŸ‘ˆ ì„¤ì • íŒŒì¼ ì„í¬íŠ¸

# --- â¬‡ï¸ ê³µí†µ ì½”ë“œ (ì‚­ì œ ë° utilsë¡œ ëŒ€ì²´) â¬‡ï¸ ---
NEWS_JSON_DIR = 'news_json'
result_filename = os.path.join(NEWS_JSON_DIR, 'truthdaily_News.json') # ğŸ‘ˆ ê³ ìœ ê°’

# 1. ê³µí†µ ìœ í‹¸ë¦¬í‹°ì—ì„œ í‚¤ì›Œë“œì™€ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
keywords, exclude_keywords = crawler_utils.load_keywords()
today = crawler_utils.get_today_string()

# 2. ê³ ìœ í•œ URL ë¦¬ìŠ¤íŠ¸
urls = [
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N1',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N2',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N3',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N4',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N5',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N6',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N7',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N8',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N9',
    'https://www.truthdaily.co.kr/news/articleList.html?sc_section_code=S1N10',
] # ğŸ‘ˆ ê³ ìœ ê°’

processed_links = set()
processed_titles = set()

# 3. is_relevant_article, get_existing_links, save_to_json í•¨ìˆ˜
# (ì´ íŒŒì¼ì—ì„œ ëª¨ë‘ ì‚­ì œ -> crawler_utilsê°€ ëŒ€ì‹  ì²˜ë¦¬)

# --- â¬‡ï¸ ì´ í¬ë¡¤ëŸ¬ë§Œì˜ 'ê³ ìœ í•œ' ë¡œì§ (ê·¸ëŒ€ë¡œ ë‘ ) â¬‡ï¸ ---

def is_within_two_days(article_time_str):
    """(ê³ ìœ  ë¡œì§) ê¸°ì‚¬ ì‹œê°„ì´ í˜„ì¬ë¡œë¶€í„° 2ì¼ ì´ë‚´ì¸ì§€ í™•ì¸"""
    try:
        # "07-30 17:43" í˜•ì‹ì„ íŒŒì‹±
        current_year = datetime.now().year
        article_datetime = datetime.strptime(f"{current_year}-{article_time_str}", "%Y-%m-%d %H:%M")
        
        # í˜„ì¬ ì‹œê°„ìœ¼ë¡œë¶€í„° 2ì¼ ì „ ê³„ì‚°
        two_days_ago = datetime.now() - timedelta(days=2)
        
        return article_datetime >= two_days_ago
    except ValueError as e:
        print(f"ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {article_time_str}, {e}")
        return False

def extract_article_details(url):
    """(ê³ ìœ  ë¡œì§) ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        img_element = soup.select_one('.article-body img')
        img_url = img_element.get('src', '') if img_element else ''
        if img_url and not img_url.startswith('http'):
            img_url = f"https://www.truthdaily.co.kr{img_url}"
        
        # ìš”ì•½/ë³¸ë¬¸ ì¼ë¶€ ì¶”ì¶œ
        content_element = soup.select_one('.article-body')
        summary = ''
        if content_element:
            paragraphs = content_element.find_all('p')
            if paragraphs:
                summary = paragraphs[0].get_text(strip=True)[:200] + "..." if len(paragraphs[0].get_text(strip=True)) > 200 else paragraphs[0].get_text(strip=True)
        
        return img_url, summary
    except Exception as e:
        print(f"ê¸°ì‚¬ ìƒì„¸ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return '', ''

def load_more_articles(session, url, page_num):
    """(ê³ ìœ  ë¡œì§) ë”ë³´ê¸° ë²„íŠ¼ì„ í†µí•´ ì¶”ê°€ ê¸°ì‚¬ ë¡œë“œ"""
    try:
        base_url = url.split('?')[0]
        params = url.split('?')[1] if '?' in url else ''
        ajax_url = f"{base_url}?{params}&page={page_num}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': url
        }
        
        response = session.get(ajax_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        return BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"ë”ë³´ê¸° ë¡œë“œ ì‹¤íŒ¨ (í˜ì´ì§€ {page_num}): {e}")
        return None

def scrape_page(url):
    """(ê³ ìœ  ë¡œì§) í˜ì´ì§€ë³„ ê¸°ì‚¬ ìˆ˜ì§‘"""
    print(f"Scraping URL: {url}")
    articles = []
    session = requests.Session()
    page_num = 1
    
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        while True:
            sections_div = soup.select_one('#sections.altlist') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
            if not sections_div:
                print("sections divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
                
            article_elements = sections_div.select('li')
            print(f"í˜ì´ì§€ {page_num}ì—ì„œ {len(article_elements)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            found_old_articles = False
            page_articles = []
            
            for element in article_elements:
                title_element = element.select_one('h2.altlist-subject a') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
                if not title_element:
                    continue
                    
                title = title_element.get_text(strip=True)
                href = title_element.get('href', '')
                full_link = href if href.startswith('http') else f'https://www.truthdaily.co.kr{href}'
                
                time_element = element.select_one('.altlist-info .altlist-info-item:last-child') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
                if not time_element:
                    continue
                    
                article_time = time_element.get_text(strip=True)
                
                if not is_within_two_days(article_time): # ğŸ‘ˆ ê³ ìœ  ì‹œê°„ ê²€ì‚¬
                    print(f"2ì¼ ì´ì „ ê¸°ì‚¬ ë°œê²¬: {title} ({article_time})")
                    found_old_articles = True
                    break
                
                # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
                is_relevant = crawler_utils.is_relevant(title, keywords, exclude_keywords)
                
                if full_link not in processed_links and is_relevant:
                    img_url, summary = extract_article_details(full_link)
                    
                    try:
                        current_year = datetime.now().year
                        dt = datetime.strptime(f"{current_year}-{article_time}", "%Y-%m-%d %H:%M")
                        published_time = dt.isoformat()
                    except ValueError:
                        published_time = article_time
                    
                    processed_links.add(full_link)
                    processed_titles.add(title)
                    
                    article_data = {
                        'title': title,
                        'time': published_time,
                        'img': img_url,
                        'url': full_link,
                        #'original_url': full_link,
                        'summary': summary
                    }
                    page_articles.append(article_data)
                    print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title} ({article_time})")
            
            articles.extend(page_articles)
            
            if found_old_articles or len(page_articles) == 0:
                print(f"ìˆ˜ì§‘ ì¤‘ë‹¨: {'2ì¼ ì´ì „ ê¸°ì‚¬ ë°œê²¬' if found_old_articles else 'ë” ì´ìƒ ê¸°ì‚¬ ì—†ìŒ'}")
                break
            
            page_num += 1
            print(f"ë‹¤ìŒ í˜ì´ì§€ {page_num} ë¡œë“œ ì¤‘...")
            soup = load_more_articles(session, url, page_num) # ğŸ‘ˆ ê³ ìœ  AJAX í˜¸ì¶œ
            
            if not soup:
                print("ë” ì´ìƒ í˜ì´ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            time.sleep(1)
            
    except Exception as e:
        print(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
    
    return articles

# --- â¬‡ï¸ main í•¨ìˆ˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) â¬‡ï¸ ---

def main():
    global processed_links, processed_titles
    
    # 1. ê³µí†µ í•¨ìˆ˜ë¡œ íŒŒì¼ ìƒì„± ë° ê¸°ì¡´ ë§í¬ ë¡œë“œ
    #crawler_utils.ensure_file_exists(result_filename)
    processed_links = crawler_utils.get_existing_links(result_filename)
    
    all_articles = []
    
    # 2. ê³ ìœ í•œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì‹¤í–‰
    for url in urls:
        articles = scrape_page(url)
        all_articles.extend(articles)
        time.sleep(2)  # ì„¹ì…˜ ê°„ ìš”ì²­ ê°„ê²© ì¡°ì ˆ
    
    # 3. ê³µí†µ í•¨ìˆ˜ë¡œ ì €ì¥
    if all_articles:
        crawler_utils.save_articles_to_json(result_filename, all_articles, today)
        print(f"ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬")
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
