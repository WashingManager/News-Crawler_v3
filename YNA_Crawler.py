# YNA_Crawler.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
import re
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import urllib.parse
import crawler_utils  # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
import crawler_config # ğŸ‘ˆ ì„¤ì • íŒŒì¼ ì„í¬íŠ¸

# --- â¬‡ï¸ ê³µí†µ ì½”ë“œ (ì‚­ì œ ë° utilsë¡œ ëŒ€ì²´) â¬‡ï¸ ---
NEWS_JSON_DIR = 'news_json'
result_filename = os.path.join(NEWS_JSON_DIR, 'yna_News.json') # ğŸ‘ˆ ê³ ìœ ê°’

# 1. ê³µí†µ ìœ í‹¸ë¦¬í‹°ì—ì„œ í‚¤ì›Œë“œì™€ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
keywords, exclude_keywords = crawler_utils.load_keywords()
today = crawler_utils.get_today_string()

# 2. ê³ ìœ í•œ URL ë¦¬ìŠ¤íŠ¸
base_urls = [
    'https://www.yna.co.kr/nk/news/politics',
    'https://www.yna.co.kr/nk/news/military',
    'https://www.yna.co.kr/nk/news/diplomacy',
    'https://www.yna.co.kr/nk/news/economy',
    'https://www.yna.co.kr/nk/news/society',
    'https://www.yna.co.kr/nk/news/cooperation',
    'https://www.yna.co.kr/nk/news/correspondents',
    'https://www.yna.co.kr/nk/news/advisory-column',
    'https://www.yna.co.kr/politics/all',
    'https://www.yna.co.kr/politics/diplomacy',
    'https://www.yna.co.kr/economy/all',
    'https://www.yna.co.kr/industry/all',
    'https://www.yna.co.kr/society/all',
    'https://www.yna.co.kr/international/all',
    'https://www.yna.co.kr/local/all',
    'https://www.yna.co.kr/culture/all'
] # ğŸ‘ˆ ê³ ìœ ê°’

processed_links = set()
processed_titles = set()

# 3. is_relevant_article, get_existing_links, save_to_json í•¨ìˆ˜
# (ì´ íŒŒì¼ì—ì„œ ëª¨ë‘ ì‚­ì œ -> crawler_utilsê°€ ëŒ€ì‹  ì²˜ë¦¬)

# --- â¬‡ï¸ ì´ í¬ë¡¤ëŸ¬ë§Œì˜ 'ê³ ìœ í•œ' ë¡œì§ (ê·¸ëŒ€ë¡œ ë‘ ) â¬‡ï¸ ---

def process_article(article, base_url):
    """(ê³ ìœ  ë¡œì§)"""
    title_element = article.select_one('span.title01') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
    title = title_element.text.strip() if title_element else ''
    if not title or title in processed_titles:
        return None
    
    link_element = article.select_one('a.tit-news') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
    href_link = link_element['href'] if link_element else ''
    if not href_link:
        return None
    
    full_link = 'https:' + href_link if href_link.startswith('//') else href_link
    parsed_url = urllib.parse.urlparse(full_link)
    clean_link = urllib.parse.urlunparse(parsed_url._replace(query=''))
    
    if clean_link in processed_links:
        return None
    
    lead_element = article.select_one('p.lead') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
    lead_full_text = lead_element.text.strip() if lead_element else ''
    
    # --- â¬‡ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ â¬‡ï¸ ---
    # p.leadì˜ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ(\n) ê¸°ì¤€ìœ¼ë¡œ 1ë²ˆë§Œ ë¶„ë¦¬
    lead_parts = lead_full_text.split('\n', 1)
    # ì²« ë²ˆì§¸ ë¶€ë¶„(ë¶€ì œëª©)ì„ ìš”ì•½ë¬¸(lead)ìœ¼ë¡œ ì‚¬ìš©
    lead = lead_parts[0].strip() if lead_parts else ''
    # --- â¬†ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ â¬†ï¸ ---

    # í‚¤ì›Œë“œ ê´€ë ¨ ì—¬ë¶€ ê²€ì‚¬ëŠ” ì›ë³¸ ì „ì²´ í…ìŠ¤íŠ¸(full_text)ë¡œ ìˆ˜í–‰
    full_text = f"{title} {lead_full_text}" 
    
    # ğŸ‘ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
    if not crawler_utils.is_relevant(full_text, keywords, exclude_keywords):
        return None
    
    time_element = article.select_one('span.txt-time') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
    published_time = ''
    if time_element:
        time_str = time_element.text.strip()
        try:
            current_year = datetime.now().year
            if '-' in time_str:  # ì˜ˆ: 04-18 20:54 # ğŸ‘ˆ ê³ ìœ  ì‹œê°„ íŒŒì‹±
                parsed_time = datetime.strptime(f"{current_year}-{time_str}", '%Y-%m-%d %H:%M')
            else:  # ì˜ˆ: 2025-04-18 20:54
                parsed_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
            published_time = parsed_time.isoformat()
        except ValueError as e:
            print(f"Invalid time format: {time_str}, Error: {e}")
            return None
    
    img_element = article.select_one('img')
    img_url = img_element.get('src', '') if img_element else ''
    
    processed_links.add(clean_link)
    processed_titles.add(title)
    print(f"Article processed: {title} ({published_time})")
    return {
        'title': title,
        'time': published_time,
        'img': img_url,
        'url': clean_link,
        #'original_url': clean_link,
        'summary': lead # ğŸ‘ˆ ìˆ˜ì •ëœ 'lead' ë³€ìˆ˜(ë¶€ì œëª©)ë¥¼ ì €ì¥
    }

def scrape_page(url, page):
    """(ê³ ìœ  ë¡œì§)"""
    print(f"Scraping URL: {url}/{page}")
    articles = []
    try:
        full_url = f"{url}/{page}" if page > 1 else url
        response = requests.get(full_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        article_elements = soup.select('ul.list01 li') # ğŸ‘ˆ ê³ ìœ  ì„ íƒì
        print(f"Found {len(article_elements)} articles")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(process_article, article, url) for article in article_elements]
            for future in as_completed(futures):
                article = future.result()
                if article:
                    articles.append(article)
        
        return articles
    except Exception as e:
        print(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({full_url}): {e}")
        return []

# --- â¬‡ï¸ main í•¨ìˆ˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) â¬‡ï¸ ---

def main():
    global processed_links, processed_titles
    
    # 1. ê³µí†µ í•¨ìˆ˜ë¡œ íŒŒì¼ ìƒì„± ë° ê¸°ì¡´ ë§í¬ ë¡œë“œ
    #crawler_utils.ensure_file_exists(result_filename)
    processed_links = crawler_utils.get_existing_links(result_filename)
    
    all_articles = []
    
    # 2. ê³ ìœ í•œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì‹¤í–‰
    for url in base_urls:
        page = 1
        while page <= 5: # ğŸ‘ˆ YNA ê³ ìœ ì˜ í˜ì´ì§€ë„¤ì´ì…˜ ë¡œì§
            articles = scrape_page(url, page)
            all_articles.extend(articles)
            if not articles:
                break
            page += 1
            time.sleep(2)
    
    # 3. ê³µí†µ í•¨ìˆ˜ë¡œ ì €ì¥
    if all_articles:
        crawler_utils.save_articles_to_json(result_filename, all_articles, today)
    else:
        print("No new articles found")

if __name__ == "__main__":
    main()
